id,url,url_id,url_title,author,upvote_ratio,score,time_created,num_gold,num_comments,category,text,main_link,flairs
1,https://www.reddit.com/r/MachineLearning/comments/e1r0ou/d_chinese_government_uses_machine_learning_not/,e1r0ou,d_chinese_government_uses_machine_learning_not,sensetime,0.96,840,2019-11-26 03:09:24,0,179,"Tech,","Link to storyThis post is not an ML research related post. I am posting this because I think it is important for the community to see how research is applied by authoritarian governments to achieve their goals. It is related to a few previous popular posts on this subreddit with high upvotes, which prompted me to post this story.Previous related stories:Is machine learning's killer app totalitarian surveillance and oppression?Using CV for surveillance and regression for threat scoring citizens in XinjiangICCV 19: The state of some ethically questionable papersHikvision marketed ML surveillance camera that automatically identifies UyghursWorking on an ethically questionnable project...The story reports the details of a new leak of highly classified Chinese government documents reveals the operations manual for running the mass detention camps in Xinjiang and exposed the mechanics of the region’s system of mass surveillance.The lead journalist's summary of findingsThe China Cables represent the first leak of a classified Chinese government document revealing the inner workings of the detention camps, as well as the first leak of classified government documents unveiling the predictive policing system in Xinjiang.The leak features classified intelligence briefings that reveal, in the government’s own words, how Xinjiang police essentially take orders from a massive “cybernetic brain” known as IJOP, which flags entire categories of people for investigation & detention.These secret intelligence briefings reveal the scope and ambition of the government’s AI-powered policing platform, which purports to predict crimes based on computer-generated findings alone. The result? Arrest by algorithm.The article describe methods used for algorithmic policingThe classified intelligence briefings reveal the scope and ambition of the government’s artificial-intelligence-powered policing platform, which purports to predict crimes based on these computer-generated findings alone. Experts say the platform, which is used in both policing and military contexts, demonstrates the power of technology to help drive industrial-scale human rights abuses.“The Chinese [government] have bought into a model of policing where they believe that through the collection of large-scale data run through artificial intelligence and machine learning that they can, in fact, predict ahead of time where possible incidents might take place, as well as identify possible populations that have the propensity to engage in anti-state anti-regime action,” said Mulvenon, the SOS International document expert and director of intelligence integration. “And then they are preemptively going after those people using that data.”In addition to the predictive policing aspect of the article, there are side articles about the entire ML stack, including how mobile apps are used to target Uighurs, and also how the inmates are re-educated once inside the concentration camps. The documents reveal how every aspect of a detainee's life is monitored and controlled.Note: My motivation for posting this story is to raise ethical concerns and awareness in the research community. I do not want to heighten levels of racism towards the Chinese research community (not that it may matter, but I am Chinese). See this thread for some context about what I don't want these discussions to become.I am aware of the fact that the Chinese government's policy is to integrate the state and the people as one, so accusing the party is perceived domestically as insulting the Chinese people, but I also believe that we as a research community is intelligent enough to be able to separate government, and those in power, from individual researchers. We as a community should keep in mind that there are many Chinese researchers (in mainland and abroad) who are not supportive of the actions of the CCP, but they may not be able to voice their concerns due to personal risk.Edit Suggestion from /u/DunkelBeard:When discussing issues relating to the Chinese government, try to use the term CCP, Chinese Communist Party, Chinese government, or Beijing. Try not to use only the term Chinese or China when describing the government, as it may be misinterpreted as referring to the Chinese people (either citizens of China, or people of Chinese ethnicity), if that is not your intention. As mentioned earlier, conflating China and the CCP is actually a tactic of the CCP.",,"Discussion,"
2,https://www.reddit.com/r/MachineLearning/comments/dxshkg/d_machine_learning_wayr_what_are_you_reading_week/,dxshkg,d_machine_learning_wayr_what_are_you_reading_week,ML_WAYR_bot,0.88,39,2019-11-17 22:00:05,0,1,"Tech,","This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.Please try to provide some insight from your understanding and please don't post things which are present in wiki.Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.Previous weeks :1-1011-2021-3031-4041-5051-6061-7071-80Week 1Week 11Week 21Week 31Week 41Week 51Week 61Week 71Week 2Week 12Week 22Week 32Week 42Week 52Week 62Week 72Week 3Week 13Week 23Week 33Week 43Week 53Week 63Week 73Week 4Week 14Week 24Week 34Week 44Week 54Week 64Week 74Week 5Week 15Week 25Week 35Week 45Week 55Week 65Week 6Week 16Week 26Week 36Week 46Week 56Week 66Week 7Week 17Week 27Week 37Week 47Week 57Week 67Week 8Week 18Week 28Week 38Week 48Week 58Week 68Week 9Week 19Week 29Week 39Week 49Week 59Week 69Week 10Week 20Week 30Week 40Week 50Week 60Week 70Most upvoted papers two weeks ago:/u/adventuringraw: original TrueSkill paper from Microsoft/u/Grimm___: http://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdfBesides that, there are no rules, have fun.",,"Discussion,"
3,https://www.reddit.com/r/MachineLearning/comments/e23ezq/p_using_stylegan_to_make_a_music_visualizer/,e23ezq,p_using_stylegan_to_make_a_music_visualizer,AtreveteTeTe,1.0,31,2019-11-26 20:54:37,0,6,"Tech,","I'm excited to share this generative video project I worked on with Japanese electronic music artist Qrion for the release of her Sine Wave Party EP.Here's the first generated video - two more coming out soon.This was created using StyleGAN and doing a transfer learning with a custom dataset of images curated by the artist.  Qrion picked images that matched the mood of each song (things like clouds, lava hitting the ocean, forest interiors, and snowy mountains) and I generated interpolation videos for each track.The tempo of the GAN evolution is controlled by a few different things:the beat of the songa system I built that incorporates live input (you can tap the keyboard to add a jump to the playback in After Effects)a keyframeable overall playback speed fader systemI've also posted some more images I created with Qrion's custom models here on my site.  There are some further StyleGAN experiments there too if you're interested.It's been fascinating to learn how to use StyleGAN like this.  As a visual effects artist, I'm over the moon with the sorts of things that are possible.  Indeed, I also wanted to shout out /u/C0D32 who shared an art-centered StyleGAN model that was really influential to me!  Thanks for that.  Also, of course, /u/gwern who posted an incredible guide to using StyleGAN.",,"Project,"
4,https://www.reddit.com/r/MachineLearning/comments/e202r7/p_i_reimplemented_stylegan_using_tensorflow_20/,e202r7,p_i_reimplemented_stylegan_using_tensorflow_20,manicman1999,0.89,34,2019-11-26 17:13:01,0,8,"Tech,","Here is a sample of 64 of the images when trained on r/EarthPorn:https://i.imgur.com/K9mU7vH.jpgHere is the code:https://github.com/manicman1999/StyleGAN-Tensorflow-2.0And finally, here is the live web demo:https://matchue.ca/p/earthhd/Enjoy!",,"Project,"
5,https://www.reddit.com/r/MachineLearning/comments/e1zub9/p_benchmarking_metric_learning_algorithms_the/,e1zub9,p_benchmarking_metric_learning_algorithms_the,VanillaCashew,0.79,18,2019-11-26 16:57:24,0,7,"Tech,","I've been researching metric learning algorithms for a while now, and in the process I discovered some issues with the field.You can read about it here: https://medium.com/@tkm45/benchmarking-metric-learning-algorithms-the-right-way-90c073a83968TL;DR:Many papers don't do apple-to-apple comparisons. They change the network architecture, embedding size, data augmentation, or just use performance-boosting tricks that aren't mentioned in their paper.Most papers don't use a validation set.Two baseline algorithms (triplet and contrastive loss) are actually competitive with the state-of-the-art, but are not presented this way in most papers.I've made a flexible benchmarking tool that can standardize the way we evaluate metric learning algorithms. You can see it here: https://github.com/KevinMusgrave/powerful_benchmarker",,"Project,"
6,https://www.reddit.com/r/MachineLearning/comments/e21fgz/r_unsupervised_attention_mechanism_across_neural/,e21fgz,r_unsupervised_attention_mechanism_across_neural,doerlbh,0.88,6,2019-11-26 18:42:17,0,2,"Tech,",,https://arxiv.org/abs/1902.10658,"Research,"
7,https://www.reddit.com/r/MachineLearning/comments/e1zdly/d_how_to_check_if_my_datasets_have_covariate/,e1zdly,d_how_to_check_if_my_datasets_have_covariate,stat_leaf,0.83,7,2019-11-26 16:23:44,0,2,"Tech,","I have 2 datasets where the covariates used are the same but the scale used are different. For example, dataset 1 could have scale for pressure parameter at 0.1, 0.2, 0.5, 0.8 and dataset 2 scale for the pressure parameter is 0.3, 0.8, 0.2, 0.9, 1.0so far I have plotted the distribution for the response variable (dependent variable) and dataset1 and dataset2 have different distributions for the response variable but if I understand correctly, covariate shift occurs when the distribution of the covariate differs between dataset1 and dataset2.How can I properly check this assumption?If I am planning to train a model on dataset1 to be tested for dataset2 with a different distribution, what are the methods that can be used ( this is a regression task) besides Neural Network?Thank you.",,"Discussion,"
8,https://www.reddit.com/r/MachineLearning/comments/e1vgpd/r_gradient_descent_happens_in_a_tiny_subspace/,e1vgpd,r_gradient_descent_happens_in_a_tiny_subspace,jarekduda,1.0,29,2019-11-26 10:09:47,0,16,"Tech,",,https://arxiv.org/pdf/1812.04754,"Research,"
9,https://www.reddit.com/r/MachineLearning/comments/e1v2q1/p_a_visual_guide_to_using_bert_for_the_first_time/,e1v2q1,p_a_visual_guide_to_using_bert_for_the_first_time,nortab,0.97,26,2019-11-26 09:24:04,0,7,"Tech,","Hi r/MachineLearning,I wrote a blog post that I hope could be the gentlest way for you to start playing with BERT for the first time;https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/It uses a lighter version of BERT (the distilled version from HuggingFace, distilBERT) to do sentence embedding, then uses Scikit Learn for Linear Regression classification. As a first exposure to BERT, I'm having people use the general trained model and not worry about fine-tuning for now. After getting people through this initial hump, I'm hoping readers would get more comfortable doing more exploration and poking around with the model and its usecases.I hope you enjoy it. All feedback/corrections are appreciated.",,"Project,"
10,https://www.reddit.com/r/MachineLearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/,e1k092,rp_talking_head_anime_from_a_single_image,pramook,0.98,313,2019-11-25 19:11:28,0,28,"Tech,","I trained a network to animate faces of anime characters. The input is an image of the character looking straight at the viewer and a pose, specified by 6 numbers. The output is another image of the character with the face posed accordingly.Play0:000:00SettingsFullscreenWhat the network can do in a nutshell.I created two tools with this network.One that changes facial poses by GUI manipulation:  https://www.youtube.com/watch?v=kMQCERkTdO0One that reads a webcam feed and make a character imitates the user's facial movement:  https://www.youtube.com/watch?v=T1Gp-RxFZwUUsing a face tracker, I could transfer human face movements from existing videos to anime characters. Here are some characters impersonating President Obama:Play0:000:00SettingsFullscreenThe approach I took is to combine two previous works. The first is the Pumarola et al.'s 2018 GANimation paper, which I use to change the facial features (closing eyes and mouth, in particular). The second is  Zhou et al.'s 2016 object rotation by appearance flow paper, which I use to rotate the face. I generated a new dataset by rendering 8,000 downloadable 3D models of anime characters.You can find out more about the project at https://pkhungurn.github.io/talking-head-anime/.",,"Research,"
11,https://www.reddit.com/r/MachineLearning/comments/e27sef/r_rigging_the_lottery_making_all_tickets_winners/,e27sef,r_rigging_the_lottery_making_all_tickets_winners,hardmaru,1.0,0,2019-11-27 01:46:06,0,1,"Tech,",,https://arxiv.org/abs/1911.11134,"Research,"
12,https://www.reddit.com/r/MachineLearning/comments/e1zw6t/d_ideas_for_interesting_eyeopening_for_beginner/,e1zw6t,d_ideas_for_interesting_eyeopening_for_beginner,titusng074,0.72,3,2019-11-26 17:01:16,0,7,"Tech,","So I'm an IB high school student and the IB curriculum requires us to write an extended essay, which is a 4000-word mini-research paper answering a research question. As I'm interested in machine learning, but CS is not available at school, I chose to write about mathematics. I explored some basic stuffs such as gradient descent and back propagation; but they are too fixed and I can't seem to formulate a question around them. Can you guys suggest some interesting mathematics in machine learning to investigate on?Also, an adult friend of mine suggest me to try ""beta distribution"", but after an hour of research I can't find the relationship between it and machine learning. Some insight will be hugely appreciated. Thanks.",,"Discussion,"
13,https://www.reddit.com/r/MachineLearning/comments/e1pvg1/p_machine_learning_systems_design_open_source/,e1pvg1,p_machine_learning_systems_design_open_source,hardmaru,0.96,53,2019-11-26 01:41:17,0,4,"Tech,","An open source book compiled by Chip Huyen. Feel free to contribute:This booklet covers four main steps of designing a machine learning system:Project setupData pipelineModeling: selecting, training, and debuggingServing: testing, deploying, and maintainingIt comes with links to practical resources that explain each aspect in more details. It also suggests case studies written by machine learning engineers at major tech companies who have deployed machine learning systems to solve real-world problems.At the end, the booklet contains 27 open-ended machine learning systems design questions that might come up in machine learning interviews. The answers for these questions will be published in the book Machine Learning Interviews.project: https://github.com/chiphuyen/machine-learning-systems-designPDF: https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf",,"Project,"
14,https://www.reddit.com/r/MachineLearning/comments/e25746/p_handwritten_text_recognition_using_convolution/,e25746,p_handwritten_text_recognition_using_convolution,spacevstab,0.67,1,2019-11-26 22:48:44,0,0,"Tech,","Convolution Seq-to-SeqInstead of using RNN with Seq-to-Seq modeling, CNN with Seq-to-Seq has been used which reduces the training and inference time. The work is novel when implemented around Dec 2018. The training and testing pipeline has been created for IAM handwitten dataset. Please provide some feedback on this project and its continuity since I would like to make further advancement and formally document the work into a article.",,"Project,"
15,https://www.reddit.com/r/MachineLearning/comments/e1z4ge/190605433v2_tackling_climate_change_with_machine/,e1z4ge,190605433v2_tackling_climate_change_with_machine,Smith4242,0.6,2,2019-11-26 16:04:34,0,2,"Tech,",,https://arxiv.org/abs/1906.05433v2,
16,https://www.reddit.com/r/MachineLearning/comments/e24e5k/d_how_can_i_elaborate_texture_and_statistic/,e24e5k,d_how_can_i_elaborate_texture_and_statistic,Samatarou,1.0,1,2019-11-26 21:58:16,0,2,"Tech,","I have a dataset 2200x34 where 1-33 column are features (texture and statistic) and 34th column is the class (0 or 1). I know my dataset is quite poor, but I splitted in 80% training set and 20% validation test.I'd like to use CNN for classification using these features, my steps are:- Splitting in training set and validation test;- Mean normalisation of features;- Reshaping training set and validation set in order to have 1760x34x1 and 440x34x1 as dimensions;- Create my model:opt = SGD(lr=0.0001)
model = Sequential()
model.add(Conv1D(16, 3, activation=""relu"", input_shape =(34,1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(2))
model.add(Conv1D(32, 3, activation=""relu""))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(512, activation=""relu""))
model.add(Dropout(0.5))
model.add(Dense(1, activation=""sigmoid""))
model.summary()
# compile the model
model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])
Sadly my model has bad performance (acc = 55% more or less and loss = 0.69). Do you have any suggestion to increase my performance? Is there something wrong?Here the model.summary()Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_3 (Conv1D)            (None, 32, 16)            64        
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 16)            64        
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 16, 16)            0         
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 14, 32)            1568      
_________________________________________________________________
max_pooling1d_4 (MaxPooling1 (None, 7, 32)             0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 224)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               115200    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 513       
=================================================================
",,"Discussion,"
17,https://www.reddit.com/r/MachineLearning/comments/e22gmc/discussion_wont_max_pooling_mess_up_a_lot_of/,e22gmc,discussion_wont_max_pooling_mess_up_a_lot_of,avdalim,0.67,1,2019-11-26 19:51:45,0,3,"Tech,","As far as I understood it, Max Pooling takes the maximum positive value and not absolute values.In this paper, optimized mechanical structures get generated with a U-Net by feeding it mechanical information like nodal displacement, element strains, and volume fractions.https://arxiv.org/ftp/arxiv/papers/1901/1901.07761.pdfMy question now is: won't Max Pooling mess up a lot of information, if it just takes the positive value and not the absolute value? Since positive elements in the strain matrix correspond to tensile strains and negative to compressive ones.",,"Discussion,"
18,https://www.reddit.com/r/MachineLearning/comments/e1tsd7/p_i_reimplemented_hyperband_check_it_out/,e1tsd7,p_i_reimplemented_hyperband_check_it_out,Deepblue129,0.86,9,2019-11-26 07:07:12,0,4,"Tech,","Hyperband is a state-of-the-art algorithm for hyperparameter tunning that focuses on resource efficiency. It does so by encorperating early-stopping into it's strategy. Here are some of the results:For more, go here: http://www.argmin.net/2016/06/23/hyperband/ I was unable to find any great implementations of hyperband, so I implemented it! Here it is: https://gist.github.com/PetrochukM/2c5fae9daf0529ed589018c6353c9f7bThe implementation is commented and documented to help ensure correctness and improve code readability.I believe I improve hyperband by allowing support for model checkpoints. The original hyperband assumed that each model was trained from scratch instead of checkpointing. We don't need to train the same model with the same hyperparameters over and over again!Finally, I also explored other improvements to hyperband like splitting based on the largest performance gap instead of splitting in half the search space every time.",,"Project,"
19,https://www.reddit.com/r/MachineLearning/comments/e1ojfo/r_understanding_the_generalization_of_lottery/,e1ojfo,r_understanding_the_generalization_of_lottery,arimorcos,0.89,19,2019-11-26 00:06:29,0,4,"Tech,","Sharing our recent blog post summarizing some of our recent work understanding the boundaries of the lottery ticket hypothesis. In particular, we make some progress towards the following questions:Do winning ticket initializations contain generic inductive biases or are they overfit to the particular dataset and optimizer used to generate them?Is the lottery ticket phenomenon limited to supervised image classification, or is it also present in other domains like RL and NLP?Can we begin to explain lottery tickets theoretically?The blog post is below:Understanding the generalization of ""lottery tickets"" in neural networksAnd the papers covered can be found here:One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizersPlaying the lottery with rewards and multiple languages: lottery tickets in RL and NLPLuck Matters: Understanding Training Dynamics of Deep ReLU NetworksStudent Specialization in Deep ReLU Networks With Finite Width and Input Dimension",,"Research,"
20,https://www.reddit.com/r/MachineLearning/comments/e1pgd0/r_adversarial_examples_improve_image_recognition/,e1pgd0,r_adversarial_examples_improve_image_recognition,hardmaru,0.86,14,2019-11-26 01:11:03,0,3,"Tech,",,https://arxiv.org/abs/1911.09665,"Research,"
21,https://www.reddit.com/r/MachineLearning/comments/e1k81s/d_artificial_life_for_ai_people/,e1k81s,d_artificial_life_for_ai_people,weiqiplayer,0.89,21,2019-11-25 19:25:20,0,8,"Tech,","AI asks fundamental questions about the nature of “intelligence”, but what about understanding life itself? Sina gives an overview of Artificial Life for AI people.https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/",,"Discussion,"
22,https://www.reddit.com/r/MachineLearning/comments/e1dpn9/r_deepfovea_neural_reconstruction_for_foveated/,e1dpn9,r_deepfovea_neural_reconstruction_for_foveated,hardmaru,0.96,103,2019-11-25 10:34:08,0,4,"Tech,","I thought this project from FB Research is a really cool work:AbstractIn order to provide an immersive visual experience, modern displays require head mounting, high image resolution, low latency, as well as high refresh rate. This poses a challenging computational problem. On the other hand, the human visual system can consume only a tiny fraction of this video stream due to the drastic acuity loss in the peripheral vision. Foveated rendering and compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery, or, if done conservatively, would provide only modest savings. In this work, we explore a novel foveated reconstruction method that employs the recent advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame. The reconstruction is done by finding the closest matching video to this sparse input stream of pixels on the learned manifold of natural videos. Our method is more efficient than the state-of-the-art foveated rendering, while providing the visual experience with no noticeable quality degradation. We conducted a user study to validate our reconstruction method and compare it against existing foveated rendering and video compression techniques. Our method is fast enough to drive gaze-contingent head-mounted displays in real time on modern hardware. We plan to publish the trained network to establish a new quality bar for foveated rendering and compression as well as encourage follow-up research.Project page / paper / video / code: https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/",,"Research,"
23,https://www.reddit.com/r/MachineLearning/comments/e1ehz1/191109723_fast_sparse_convnets/,e1ehz1,191109723_fast_sparse_convnets,ekelsen,0.94,54,2019-11-25 11:59:30,0,8,"Tech,",,https://arxiv.org/abs/1911.09723,
24,https://www.reddit.com/r/MachineLearning/comments/e1kfy2/r_191106786_stagewise_knowledge_distillation/,e1kfy2,r_191106786_stagewise_knowledge_distillation,akshayk07,0.86,5,2019-11-25 19:39:53,0,5,"Tech,",,https://arxiv.org/abs/1911.06786,"Research,"
25,https://www.reddit.com/r/MachineLearning/comments/e1rpzq/d_where_are_the_good_machine_learning_books_for/,e1rpzq,d_where_are_the_good_machine_learning_books_for,Ctown_struggles00,0.5,0,2019-11-26 04:04:18,0,4,"Tech,","For beginners there's PRML by Bishop and maybe Understanding Machine Learning by Shai2 but for advanced readers or those interested in the deep learning and GAN research landscape (and how to apply it) there really isn't anything good out there.I personally don't like Goodfellow's Deep Learning book. I wish there was a good deep-dive out there but there just isn't what I need.I think Andrej Karpathy is a good writer, kind of wish he could throw something together!",,"Discussion,"
26,https://www.reddit.com/r/MachineLearning/comments/e191z6/p_nboost_boost_elasticsearch_search_relevance_by/,e191z6,p_nboost_boost_elasticsearch_search_relevance_by,colethienes,0.94,79,2019-11-25 03:12:54,0,9,"Tech,","Hi Everyone!New to reddit, but I'd like to share a project I've been working on called NBoost. It's essentially a proxy for search APIs (e.g. Elasticsearch) that reranks search results using finetuned models (e.g. BERT).Check out our medium article or github to learn more!It's main features include:Super easy to set up (you can just pip install nboost)Easy, non-invasive integration with Elasticsearch and potentially other search APIs.Finetuned models are plugins (you can swap them in and out).Fast and scaleable (written at the lowest level possible)",,"Project,"
27,https://www.reddit.com/r/MachineLearning/comments/e1gssh/d_how_do_you_expect_ml_to_transition_over_to/,e1gssh,d_how_do_you_expect_ml_to_transition_over_to,nocomment_95,0.91,10,2019-11-25 15:31:25,0,38,"Tech,","First off I am not an ML engineer. I am am embedded software engineer working mostly in safety critical systems. So if there are some dumb assumptions here don't crucify me. One of the biggest things that strikes me about ML is it's black box nature. We can't ask the machine how it made a descision, in fact I've heard claims that we shouldn't because it would inject human bias into the system. For things like data scraping and image recognition that seems fine, but I can't imagine having a conversation at my work go like this:""X failed, people died. Go figure out how and fix it.Sorry boss I can retrain the model with this new outcome but I can't tell you why it broke or guarantee to any degree of certainty it won't happen again""That just wouldn't fly. Is there something I'm missing?",,"Discussion,"
28,https://www.reddit.com/r/MachineLearning/comments/e1d447/d_imagenet_classification_training_fullresolution/,e1d447,d_imagenet_classification_training_fullresolution,tsauri,0.83,20,2019-11-25 09:26:45,0,14,"Tech,","Are there papers that do so? ImageNet images have various resolutions. Since we already have adaptive pooling techniques such as global average pooling, attention-based pooling, etc. that support variable resolution, should it be possible?Random fixed size crop and resize seems hacky.",,"Discussion,"
29,https://www.reddit.com/r/MachineLearning/comments/e10b5x/d_what_happened_to_the_thread_on_taiwan_and_iccv/,e10b5x,d_what_happened_to_the_thread_on_taiwan_and_iccv,arkady_red,0.95,401,2019-11-24 17:04:43,0,186,"Tech,","As per subject, wasn't there a thread on that yesterday? I can't find it anymore. Was it mowed down by moderators?",,"Discussion,"
30,https://www.reddit.com/r/MachineLearning/comments/e1k8cc/d_datapoisoning_and_trojan_attacks_at_training/,e1k8cc,d_datapoisoning_and_trojan_attacks_at_training,niklongstone,1.0,3,2019-11-25 19:25:50,0,1,"Tech,",I would like to know anyone's opinion on this.Recent work has identified that classification models implemented as neural networks are vulnerable to data-poisoning and Trojan attacks at training time.Source: Attacks on Deep Reinforcement Learning Agents : https://arxiv.org/abs/1903.06638Is it a real threat?How the risk can be identified from someone that just uses the model without access to its source or training data (i.e. prepare a set of tests)?,,"Discussion,"
31,https://www.reddit.com/r/MachineLearning/comments/e1jzkz/discussiond_gradient_norm_tracking/,e1jzkz,discussiond_gradient_norm_tracking,pubertat,1.0,2,2019-11-25 19:10:16,0,3,"Tech,","Are there any best practices on how one should track gradient norms during training? Surprisingly, I haven't been able to find much reliable information on it, except the classical Glorot's paper.My current approach is to track 2-norm of weights raw gradients. However, I don't have any practical intuition on which values should make me worried. Tracking the actual weight updates (e.g adjusted by Adam) makes make much more sense, but I haven't seen anyone doing so.A few words why am I concerned: I'm working on some exotic NN architecture for 3D, where different architecture choices implicate gradient behavior drastically, up to blow up.",,"Discussion,"
32,https://www.reddit.com/r/MachineLearning/comments/e1dxvl/d_image_synthesis_before_gans/,e1dxvl,d_image_synthesis_before_gans,WoahCanyonero,0.77,9,2019-11-25 10:59:31,0,16,"Tech,","I'm writing my master's thesis on image generation and was wondering: which other methods were/are used to synthesize images, aside from GANs? Any time I look up ""image synthesis"" I only find GAN tutorials, and excluding GANs seems to bring up photon scanning.",,"Discussion,"
33,https://www.reddit.com/r/MachineLearning/comments/e1elhy/d_use_ai_to_turn_low_poly_world_into/,e1elhy,d_use_ai_to_turn_low_poly_world_into,AlexaPomata,0.63,5,2019-11-25 12:09:41,0,25,"Tech,","Hi,I wonder why we are still trying to mimic photorealistic world by counting every reflection, polygon, tracing every ray and so on. Shouldn't it be done in such manner that AI is just doing the job basing on photos and low polygon input like here https://assetstore.unity.com/packages/3d/characters/animals/poly-art-forest-set-128568 Also all other games like Zelda BOTW, Team Fortress 2 or even Fortnite could be easily turned by AI into photorealistic env. Shouldn't we start thinking about doing AI accelerators (like first 3dfx cards) for enriching low polygonic world's generated easily by most commodity hardware? I guess even ray tracing could be made by ML. I believed that future belongs to generating world by AI not by tricky mathematic graphics algorithms. Especially that in future it is easier to go from such trained networks into environment where instead of heaving an output on display, the output would be ""drawn"" directly in human brain through neural-connectivity. Also AI is able to properly handle cases where object is moving fast or turning around.Cheers, Alexa",,"Discussion,"
34,https://www.reddit.com/r/MachineLearning/comments/e1jyyd/d_deep_machine_learning/,e1jyyd,d_deep_machine_learning,Rioghasarig,0.4,0,2019-11-25 19:09:09,0,5,"Tech,","So, I'm a big fan of Lex Fridman's deep learning podcast. A big ago I watched one he did with Ian Goodfellow.At the start of the interview Goodfellow describes how deep learning methods are distinguished by the fact that it involves a bunch of computations done in sequence rather than in parallel. (You can watch the video to get a better idea of what he was talking about).Does anyone have any other examples of machine learning techniques that you feel fit his description of being deep? Just curious about this.",,"Discussion,"
35,https://www.reddit.com/r/MachineLearning/comments/e1jxin/p_webpage_data_extraction_using_image/,e1jxin,p_webpage_data_extraction_using_image,JsonPun,0.67,1,2019-11-25 19:06:47,0,3,"Tech,","I am working on creating something that can detect and ideally extract information from a job posting.I have some questions around the data I am using. I currently crawl websites and take screenshots of their career pages. These screenshots vary in dimensions due to the length of the website.Disclaimer, I am not a ML Pro. I am self taught everything and currently using Google's AutoML Services for training my model.My Questions:Should I use these long/large images? Or is it better to cut them in half and then feed it to the AI. With the large images when I zoom in I can see everything fine for labeling. When not zoomed in, it can be hard to make things out.How small should labels be? Google allows the smallest to be 8 pixels by 8 pixels. If they can be big I can use the large images and just zoom in?Is there a way to give context to the classifier/object detector? I realized when I evaluate a job posting I get context from the url and other words on the page that it doesn't get since it only sees a screenshot.Should I try to label every element on the page? if yes, In a high level way or granular?Any other hints or tips I should think about to solve this problem?My Attempts/ApproachesAttempt 1: Object DetectionMy first attempt was to perform object detection on screen shots that were cut down to ~2,000 pixels. I then labeled most of the content on the page with labels like: Header, Footer, Section, Heading, SubHeading, Job Title, Job Posting, Paragraph, Section Heading, Section SubHeading.Results :Total images: 183Test items: 17Total objects: 244Object to image avg: 14.35Precision: 91.43% (Using a score threshold of 0.508)Recall: 13.11% (Using a score threshold of 0.508)Average precision: 0.171 at 0.5 IoUConclusion: Object detection needs many more images, also the labels I provided were not concrete enough. Looking back I found the definitions for certain things to be vague. For example I was using the label heading, subheading and job title. Well sometimes the heading is also a job title, but I would only mark it as job title. Thinking about it from the computers perspective how will it know a heading from a job title? There is not much there visually for it to grab onto. This lead me to cut the images down to a height of 2,000 pixels so I could see each element more clearly.The problem here is do I try to label every HTML element?Attempt 2 Object ClassificationMy second try was to use image classification to determine if I was on a job posting page, then if true use another model to extract the data.My first model1 resultsTotal images: 85Test items: 9Precision: 77.78%Recall: 77.78%My second model2 resultsTotal images: 484**Test items:**55Precision: 90.7%Recall: 70.91%These results were more in-line with what I had thought. When looking at the overall page there over and over there becomes a familiar pattern with what a job posting looks like.Final Attempt - Object Detection:I am now trying again with an object detection model, that is trained only on job posting's, I think this will do better as it only has 3 labels, Job Title, Job Location and Apply Button. I wanted to include a label for: Responsibilities, Qualifications, skills, bonus, ect... but came back to the fact that there is not much for it to grab onto...as I find these in the posting by reading.Model currently in training...Final NotesI believe the correct way for me to do this problem would be to train the AI on the html code, but I am using google's automl services so I dont know how/if that is possible. I was thinking about using/combining different types of data/techniques since there is information in the URL and code that I'm not leveraging. Perhaps apply NLP to the URLS?Thanks for checking out my project any thoughts are appreciated.",,"Project,"
36,https://www.reddit.com/r/MachineLearning/comments/e13qhb/r_a_simple_module_consistently_outperforms/,e13qhb,r_a_simple_module_consistently_outperforms,stopwind,0.87,56,2019-11-24 21:02:39,0,46,"Tech,","In the paper: MUSE: Parallel Multi-Scale Attention for Sequence to Sequence LearningWe delve into three questions in sequence to sequence learning:Is attention alone good enough？Is parallel  representation learning applicable to sequence data and tasks?How to design a module that combines both inductive bias of convolution and  self-attention？We find that there are shortcomings in stand-alone self-attention, and present a new module that maps the input to the hidden space and performs the three operations of self-attention, convolution and nonlinearity in parallel, simply stacking this module outperforms all previous models including Transformer (Vasvani et al., 2017) on main NMT tasks under standard setting.Key features:First successfully combine convolution and self-attention in one module for  sequence tasks by the proposed shared projection,SOTA on three  main translation datasets, including WMT14 En-Fr, WMT14 En-De and IWSLT14  De-En,Parallel  learn sequence representations and thus have potential for acceleration.Quick links:Arxiv : pdf;Github : Code, pretrained models, instructions for training are all available.Main results:Abstract:In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism.",,"Research,"
37,https://www.reddit.com/r/MachineLearning/comments/e0yvok/p_hypertunity_a_library_for_hyperparameter/,e0yvok,p_hypertunity_a_library_for_hyperparameter,gdikov,0.98,119,2019-11-24 15:04:10,0,22,"Tech,","I would like to share my pet project, Hypertunity, a Python library for black-box hyperparameter optimisation. It's main features are:Bayesian Optimisation using Gaussian process regression by wrapping GPyOpt;Native support for random and grid search;Visualisation of the results in Tensorboard using the HParams plugin;Scheduled, parallel execution of experiments using joblib;Also possible to schedule jobs on Slurm.For the full set of features, check out the docs.Your feedback is very much appreciated!",,"Project,"
38,https://www.reddit.com/r/MachineLearning/comments/e11t34/p_pysnn_spiking_neural_network_framework_built_on/,e11t34,p_pysnn_spiking_neural_network_framework_built_on,DontShowYourBack,0.91,47,2019-11-24 18:52:06,0,15,"Tech,","Hi everyone!Recently a friend and I have been working on a new Python library for machine learning with Spiking Neural Networks (SNNs), called PySNN, which is built on top of PyTorch. We feel it is time to share it with more people, and hopefully get good feedback and contributions:)!Our goal for  PySNN was to make a truly modular framework for machine learning with SNNs, while staying as close to PyTorch as possible. All of the existing frameworks either operate more like simulators for neuroscientific research, or use relatively fixed network and training/evaluation loop designs. PySNN, on the other hand, consists of building blocks for neurons, connections, and learning rules which the user can combine in their desired way. It even allows for mixing learning rules or training only specific parts of the network.Furthermore, since PySNN consists of just the basic elements, the framework is lightweight and allows for easy extension. Because of its tight integration with PyTorch it fully supports GPU acceleration, batching of samples, and supports tools like the jit compiler and graph tracing for TensorBoard.There are still many improvements and extensions that can be made, so feel free to have a look and send out a pull request! We will be very active in helping with any issues! https://github.com/BasBuller/PySNNWe are looking forward to your comments and suggestions!:)",,"Project,"
39,https://www.reddit.com/r/MachineLearning/comments/e1fp0d/p_how_can_i_build_this_simple_textbased_ml_tool/,e1fp0d,p_how_can_i_build_this_simple_textbased_ml_tool,ventura__highway,0.5,0,2019-11-25 13:56:38,0,4,"Tech,","Hello everyone!I work with spreadsheets a lot, doing tasks manually that are just a bit too complex for rules, but I believe they certainly fall into what ML can handle. In a nutshell, I spend 2+ hours a day going through company names, removing legal terms like ""LLC"" or ""Limited"", and humanizing them.For instance, I have a spreadsheet with company names and emails.Company NameEmail AddressConcur Recruitment Limited - 02476 668 204sconvery@concurengineering.co.ukConfluent Technology Groupmark.anderson@confluentgroup.comConstruction Maintenance and Allied Workersdonmelanson@cmaw.caThese would become (currently by hand):Company NameEmail AddressConcur Engineeringsconvery@concurengineering.co.ukConfluentmark.anderson@confluentgroup.comCMAWdonmelanson@cmaw.caWhat we're doing here is:Shorting names to their essenceRemoving legal terms and wordsLooking at domain names (in email addresses) as a clue for the ""most human name""Now, I very well believe this is something Google Cloud has capabilities for. Given the lack of programming involved with Google Cloud ML (and its potential integration with Google Sheets), I'd imagine it's the best vehicle for this tool.Some questions before I embark upon this journey:Would your recommend I use Google Cloud ML or another tool?How much data would you imagine would be necessary to train this tool? (uncleaned spreadsheets and cleaned spreadsheets)Am I critically misunderstanding something here? This is pretty much my first time practically applying ML.Thank you very much for all your help!",,"Project,"
40,https://www.reddit.com/r/MachineLearning/comments/e12eel/d_what_do_you_see_as_the_most_promising/,e12eel,d_what_do_you_see_as_the_most_promising,p6m_lattice,0.86,15,2019-11-24 19:33:40,0,19,"Tech,","I often read from ML researchers, but more from computational cognitive scientists, that humans are able to generalize patterns from only a few data points or use ""rich, informative priors"" even as children, and how that is very important for us as cognitive beings that sets us apart from the current neural network approaches to RL used today.I'm also not entirely convinced that the current neural net paradigm with the McCulloch–Pitts-esque neurons is ever going to become sample efficient enough for real-world reinforcement learning tasks. It seems like despite our best efforts to increase sample efficiency in NN techniques, the most impressive results still use hundreds of thousands or more simulations/data points that could be infeasible to implement for any sufficiently complex real-world environments.That being said, what approaches are you most excited for in reducing sample efficiency in reinforcement learning or in neural network techniques in general?",,"Discussion,"
41,https://www.reddit.com/r/MachineLearning/comments/e149u6/d_anybody_know_an_implementation_of_fchollets/,e149u6,d_anybody_know_an_implementation_of_fchollets,daanvdn,0.86,10,2019-11-24 21:39:33,0,2,"Tech,",This is the paper: Information-theoretical label embeddings for large-scale image classificationhttps://arxiv.org/abs/1607.05691,,"Discussion,"
42,https://www.reddit.com/r/MachineLearning/comments/e0up0p/r_reimplementation_of_hyperspherical_prototype/,e0up0p,r_reimplementation_of_hyperspherical_prototype,ACTBRUH,0.93,60,2019-11-24 07:02:08,0,5,"Tech,","Link to paper: https://arxiv.org/abs/1901.10514Link to my reimplementation: https://github.com/Abhishaike/HyperProtoNetReproduceThis is a Pytorch reimplementation of the NeurIPS 2019 paper ‘Hyperspherical Prototype Networks’ in Pytorch. This paper proposes an extension to Prototype Networks, in which the prototypes are placed a priori with large margin separation, and remain unchanged during the training/testing process of the model. The paper suggests that this extension allows for more flexible classification, regression, and joint multi-task training of regression/classification, and with higher accuracy compared to typical Prototype Networks.This repo includes reproduced benchmarks for most of their datasets. Largely the same accuracy/error, but quite off on CIFAR-100 (not ImageNet-200 though for some reason), so it's possible this is an issue on my end.I also found their use of SGD for prototype creation to be unusual, considering that, the way they phrased the prototype problem, it seems like a job more for a constrained optimization algorithm. Alongside the SGD implementation (which are used for the included benchmarks), I added in two other optimization algorithms, one unconstrained (BFGS) and one constrained (SLSQP). These didn't seem to change the results much.This is my first reimplementation of a paper, so any critiques would be great!",,"Research,"
43,https://www.reddit.com/r/MachineLearning/comments/e0z7xs/discussion_hyperparameters_for_word2vec_for_sms/,e0z7xs,discussion_hyperparameters_for_word2vec_for_sms,conradws,0.78,8,2019-11-24 15:34:27,0,12,"Tech,","Hey all,Working at a small startup, and we have extracted 33 million text messages from our users. We plan to create a model to classify different types of sms relevant to us.First step is to create a Word 2 Vector dictionary for EDA and clustering and possibly to use these embeddings for classification further down the line .Just wanted some guidance about the hyperparameters for the gensim's Word2Vec.The corpus is 33 million sms, average sms length is 16 words and the vocab size is 1.5 million.I used the following hyperparameters and obtained decent results but just wanted to know if I'm doing anything wrong that could be hampering the model from performing even better:Cbow, window = 4, vector size = 125,  iterations =10, workers = 5, min_count= 4.Furthermore does anyone have any tips on how to evaluate the embeddings ( other than checking that the similarity for a small set of words makes sense) so that I can fine-tune these hyperparameters?And final question ( I promise) Would it possible or recomendable to take a pre trained Word2Vec model and improve on it by giving it the sms data so that it learns new words like slang and typos without losing its overall knowledge of the language?Thanks so much for your time in reading.",,"Discussion,"
44,https://www.reddit.com/r/MachineLearning/comments/e0vbhq/d_aistats_2020_reviews/,e0vbhq,d_aistats_2020_reviews,donb1988,0.83,23,2019-11-24 08:12:56,0,11,"Tech,","AISTATS 2020 reviews are marked for release on Nov 24, 2019. Here's a thread to discuss this year's reviews. Godspeed, everyone!",,"Discussion,"
45,https://www.reddit.com/r/MachineLearning/comments/e0j9cb/p_2000x_faster_rapids_tsne_3_hours_down_to_5/,e0j9cb,p_2000x_faster_rapids_tsne_3_hours_down_to_5,danielhanchen,0.73,189,2019-11-23 16:27:45,0,28,"Tech,","TSNE is a very popular data visualization algorithm used alongside PCA and UMAP.Sklearn's TSNE is very effective for small datasets, but on the 60,000 MNIST Digits dataset, expect to wait 1 hour. With RAPIDS cuML, TSNE on MNIST runs in 3 seconds!On 200,000 rows, Sklearn takes a whopping 3 hours, whilst RAPIDS takes 5 seconds! (2,000x faster). Figure 1. cuML TSNE on MNIST Fashion takes 3 seconds. Scikit-Learn takes 1 hour. Check out my blog showcasing how cuML achieves this massive performance boost, and how NVIDIA GPUs can help scientists and engineers save their precious time. https://medium.com/rapids-ai/tsne-with-gpus-hours-to-seconds-9d9c17c941db Figure 2. TSNE used on the 60,000 Fashion MNIST dataset (3 seconds)Give cuML a try! You might know me as the author of HyperLearn, and I can say cuML is the gold standard package for machine learning on GPUs! https://github.com/rapidsai/cumlLinear Regression, UMAP, K-Means, DBSCAN etc are all sped up on the GPU! If you have any questions, feel free to ask! Table 1. cuML’s TSNE time running on an NVIDIA DGX-1 with using 1 V100 GPU. Finally, a big drawback of current GPU implementations is its memory consumption. With cuML TSNE, it uses 30% less GPU memory! In a future release, this will be shaved by 33% again to a total of 50% memory reductions! It will also support PCA initialization.Note: My aim is to showcase how GPUs fair against the common use case. Many scientists and engineers use Sklearn's TSNE, so Sklearn is compared against.",,"Project,"
46,https://www.reddit.com/r/MachineLearning/comments/e0q6y7/d_nonstudents_whats_your_day_job/,e0q6y7,d_nonstudents_whats_your_day_job,Ctown_struggles00,0.91,19,2019-11-24 00:36:42,0,66,"Tech,",What kind of work do you guys do?,,"Discussion,"
47,https://www.reddit.com/r/MachineLearning/comments/e10e40/p_a_chessgoshogi_model_that_passes_the_turing/,e10e40,p_a_chessgoshogi_model_that_passes_the_turing,Pawngrubber,0.42,0,2019-11-24 17:11:01,0,17,"Tech,","I want to build a model for Chess/Go/Shogi that is trained and tested on real players, and I want it to pass the Turing test.  I don't want my model to play the best move in a position, I want it to play the move that a person would play (of a certain strength, time control, etc..).It's easy to make this a classification problem and train a CNN on a one-hot encoded policy of actual moves played.  The only problem is, without some kind of look-ahead algorithm (MCTS for example) the model fails to learn sequences that require multiple moves, such as tactics.However, current MCTS/alpha-beta/minimax models require evaluation of leaf nodes.  I don't have a way to shape the reward to an evaluation of a leaf node.  So my question: how would I incorporate a look-ahead algorithm in an imitation learning problem like this?",,"Project,"
48,https://www.reddit.com/r/MachineLearning/comments/e0x56q/need_some_advice_for_training_bert_to_classify/,e0x56q,need_some_advice_for_training_bert_to_classify,shstan,0.57,1,2019-11-24 11:59:27,0,9,"Tech,","Basically, so far, I have been trying to train BERT on a very long document by cutting start, middle , and end sections of article so it could be fit into the limited input dimension of 512. However; the performance has been dismal for most of the time. So far, I am not sure if using LSTM+GRU was a better approach than this. But are there other ways to train it than just cutting up the article? When I googled for an alternative approach, I couldn’t find much...",,"Project,"
49,https://www.reddit.com/r/MachineLearning/comments/e0ijkd/d_generating_visualizations_for_network/,e0ijkd,d_generating_visualizations_for_network,ssd123456789,0.92,39,2019-11-23 15:29:33,0,16,"Tech,",Deep learning papers often have very good diagrams of their architectures. Does anyone know of tools that can be used to generate these sorts of diagrams. I'm not looking for automatically generated diagrams.So my question is this:What kind of software do people use to make nice looking Visualizations for their network architecture. A really nice example is the pointnet architecture. So does anyone know what was or could have been used to generate that architecture diagram.,,"Discussion,"
50,https://www.reddit.com/r/MachineLearning/comments/e0s430/r_190800156_deep_gaussian_networks_for_function/,e0s430,r_190800156_deep_gaussian_networks_for_function,AforAnonymous,0.6,3,2019-11-24 03:08:45,0,2,"Tech,",,https://arxiv.org/abs/1908.00156,"Research,"
51,https://www.reddit.com/r/MachineLearning/comments/e10e1h/discussion_the_wolf_of_silicon_valley_data/,e10e1h,discussion_the_wolf_of_silicon_valley_data,stensool,0.36,0,2019-11-24 17:10:50,0,2,"Tech,",A satirical piece I wrote on the parallels between machine learning engineers and Wall Street traders:https://towardsdatascience.com/the-wolf-of-silicon-valley-150e5f501216,,"Discussion,"
52,https://www.reddit.com/r/MachineLearning/comments/e03azf/n_china_forced_the_organizers_of_the/,e03azf,n_china_forced_the_organizers_of_the,Only_Assist,0.94,825,2019-11-22 17:28:14,0,224,"Tech,","Link: http://www.taipeitimes.com/News/front/archives/2019/11/02/2003725093The Ministry of Foreign Affairs yesterday protested after China forced the organizers of the International Conference on Computer Vision (ICCV) in South Korea to change Taiwan’s status from a “nation” to a “region” in a set of slides.At the opening of the conference, which took place at the COEX Convention and Exhibition Center in Seoul from Tuesday to yesterday, the organizers released a set of introductory slides containing graphics showing the numbers of publications or attendees per nation, including Taiwan.However, the titles on the slides were later changed to “per country/region,” because of a complaint filed by a Chinese participant.“Taiwan is wrongly listed as a country. I think this may be because the person making this chart is not familiar with the history of Taiwan,” the Chinese participant wrote in a letter titled “A mistake at the opening ceremony of ICCV 2019,” which was published on Chinese social media under the name Cen Feng (岑峰), who is a cofounder of leiphone.com.The ministry yesterday said that China’s behavior was contemptible and it would not change the fact that Taiwan does not belong to China.Beijing using political pressure to intervene in an academic event shows its dictatorial nature and that to China, politics outweigh everything else, ministry spokeswoman Joanne Ou (歐江安) said in a statement.The ministry has instructed its New York office to express its concern to the headquarters of the Institute of Electrical and Electronics Engineers, which cosponsored the conference, asking it not to cave in to Chinese pressure and improperly list Taiwan as part of China’s territory, she said.Beijing has to forcefully tout its “one China” principle in the global community because it is already generally accepted that Taiwan is not part of China, she added.As China attempts to force other nations to accept its “one China” principle and sabotage academic freedom, Taiwan hopes that nations that share its freedoms and democratic values can work together to curb Beijing’s aggression, she added.",,"News,"
53,https://www.reddit.com/r/MachineLearning/comments/e0pir2/dwhy_isnt_chainer_more_useddiscussed/,e0pir2,dwhy_isnt_chainer_more_useddiscussed,TheAlgorithmist99,0.64,5,2019-11-23 23:47:20,0,9,"Tech,","Pretty much what it says in the title, but to elaborate on the two questions:Why isn't Chainer more used? It seems to have pioneered some nice ideas like model subclassing, has many interesting ""sub-libraries"", like ChainerCV and ChainerRL, but I think it is barely used outside Japan (not exactly sure if they use it a lot either)And then in the same vein, why is it not more discussed when talking about Deep Learning frameworks? We see a lot of comparison between Pytorch and Tensorflow, then maybe some MxNet and new players (Jax, Halide) and non-python frameworks (mostly Julia's), but Chainer almost seems to be ignored in most of these discussions.(Also feel welcome to comment on pretty much the same questions but regarding Cupy vs Numba or similar)",,"Discussion,"
54,https://www.reddit.com/r/MachineLearning/comments/e0jepq/p_predict_figure_skating_world_championship/,e0jepq,p_predict_figure_skating_world_championship,seismatica,0.72,5,2019-11-23 16:39:49,0,0,"Tech,","I'm trying to predict the ranking of figure skaters in the annual world championship by their scores in earlier competition events in the season. The obvious method to do is by average the scores for each skater across past events and rank them by those averages. However, since no two events are the same, the goal for my project is to separate the skater effect, the intrinsic ability of each skater, by the event effect, how an event influence the score of a skater.I've previously posted on Reddit my attempts to do so using simple linear models, which you can read on Medium part 1 of my project. These models will output a latent score for each skater that we can use to rank them.However, another approach to learn the latent scores of skater is to think of factorizing the event-skater matrix of raw scores in the season into a skater-specific matrix and an event-specific matrix of latent scores that multiply together to approximate the raw score. Therefore, this is exactly the same as the [matrix factorization in recommender systems](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)), but with user=skater, item=event, and rating=raw score.As a result, I used a variant of the famous FunkSVD algorithm to learn the latent scores of skater. In part 2 of my project, I tried finding just a single latent score for each skater, and rank skaters by those scores. Next, in part 3, I learned multiple latent factors for each skater using the same FunkSVD method. Since I'm implementing it from scratch, I try using various implementations of the algorithm: from a naive approach using for loop, to one using numpy broadcasting, and one using matrix multiplication, and benchmark them both in time and space complexity.However, one major problem with multiple factors is that it's hard to know which factor to rank skater with. Thankfully, the ranking metric I use in the project (Kendall's tau) allows me to build a simple logistic regression model to combine these scores to rank the skaters. This can be done with pairwise score differences in each factor as predictors, and the world championship ranking itself as the response. I later learned that this belong to the pairwise learning-to-rank methods often encountered in information system, and you can read my implementation of it in part 4.However, the result at the end of this part was not very encouraging, likely due to the way that I use FunkSVD to train the latent factors. Therefore, I part 5, I modified my FunkSVD implementation to solve this problem, by training the factors in sequence instead of all at once. I then discovered afterward that Mr Funk also originally trained all of his factors in sequence, so I should have read his work more carefully at the start!You can see all the code I used for my project in the Github repo. I'm more than happy to receive any questions or feedback from you guys on my project!",,"Project,"
55,https://www.reddit.com/r/MachineLearning/comments/e0akbb/d_iclr_reviewers_and_making_the_ml_community/,e0akbb,d_iclr_reviewers_and_making_the_ml_community,watercannon123,0.87,35,2019-11-23 01:59:40,0,18,"Tech,","I'm reviewing for ICLR myself, so I know reading the revised papers and carefully reading all the lengthy rebuttals feels like a terrible time-sink, but to everyone else who's also reviewing: please remember that most authors have spent an enormous time and effort in their submissions.I've noticed that many reviews have already been updated after the rebuttal period but it seems that most if not all miss key points that are addressed in the rebuttal or in the revised paper. There's an option to compare revisions which highlights the changes -- please use this feature, as some authors address points in the revision but don't mention it explicitly in the rebuttal (this actually happened for all papers I'm reviewing). I submitted a paper myself and my reviews were shorter than last year, and I also have the feeling that my rebuttal wasn't carefully read by the reviewers who updated their reviews.I also know that many reviewers this year are reviewing for the first time, but please do make an effort to spend some time going over rebuttals and revisions. You're now part of the ML academic community -- try to make it better, we need it especially now that the many of the highest-rated papers have extremely short reviews with low confidence scores, including reviews as short as 20 words.TLDR: we all know there are not enough reviewers and way too many submissions. while reviewing for free can be frustrating, the community depends on us and the job includes being thoughtful and reading rebuttals/revisions carefully",,"Discussion,"
56,https://www.reddit.com/r/MachineLearning/comments/e03m49/p_cleanlab_accelerating_ml_and_deep_learning/,e03m49,p_cleanlab_accelerating_ml_and_deep_learning,cgnorthcutt,0.93,38,2019-11-22 17:50:17,0,8,"Tech,","Hey folks. Today I've officially released the cleanlab Python package, after working out the kinks for three years or so. It's the first standard framework for accelerating ML and deep learning research and software for datasets with label errors. cleanlab has some neat features:If you have model outputs already (predicted probabilities for your dataset), you can find label errors in one line of code. If you don't have model outputs, its two lines of code.If you're a researcher dealing with datasets with label errors, cleanlab will compute the uncertainty estimation statistics for you (noisy channel, latent prior of true labels, joint distribution of noisy and true labels, etc.)Training a model (learning with noisy labels) is 3 lines of code.cleanlab is full of examples -- how to find label errors in ImageNet, MNIST, learning with noisy labels, etc.Full cleanlab announcement and documentation here: [LINK]GitHub: https://github.com/cgnorthcutt/cleanlab/ PyPI: https://pypi.org/project/cleanlab/As an example, here is how you can find label errors in a dataset with PyTorch, TensorFlow, scikit-learn, MXNet, FastText, or other framework in 1 line of code.# Compute psx (n x m matrix of predicted probabilities)# in your favorite framework on your own first, with any classifier.# Be sure to compute psx in an out-of-sample way (e.g. cross-validation)# Label errors are ordered by likelihood of being an error.# First index in the output list is the most likely error.from cleanlab.pruning import get_noise_indicesordered_label_errors = get_noise_indices(s=numpy_array_of_noisy_labels,psx=numpy_array_of_predicted_probabilities,sorted_index_method='normalized_margin', # Orders label errors)cleanlab logo and my cheesy attempt at a slogan.P.S. If you happen to work at Google, cleanlab is incorporated in the internal code base (as of July 2019).P.P.S. I don't work there, so you're on your own if Google's version strays from the open-source version.",,"Project,"
57,https://www.reddit.com/r/MachineLearning/comments/e00qdj/p_windhive_ml_based_coding_assistant_to_boost/,e00qdj,p_windhive_ml_based_coding_assistant_to_boost,crystal_alpine,0.84,57,2019-11-22 14:07:03,0,16,"Tech,","Hi r/MachineLearning, we were tired of constantly having to search Google, StackOverflow, and GitHub for code examples and API documentation when writing code. We built WindHive.ai, a smart coding assistant that provides you contextually relevant code snippets and docs directly from your editor!WindHive does this by using machine-learned code representations/embeddings. We have trained neural networks on hundreds of publicly available code repositories to create embeddings for the task of deciding which snippets of code would be most useful to show you at any given time. Using these embeddings, we are able to index and search through tens of thousands of code snippets and show you exactly what you need.We believe WindHive can help you increase your programming productivity and avoid reinventing the wheel! Please to to windhive.ai to find out more and share your feedback with us!",,"Project,"
58,https://www.reddit.com/r/MachineLearning/comments/e0ju09/d_something_like_leetcode_for_to_learn_mlpython/,e0ju09,d_something_like_leetcode_for_to_learn_mlpython,Knackmanic,0.29,0,2019-11-23 17:10:50,0,7,"Tech,",I'd like to become more proficient in Python and ML. Have you found a method to train yourself?,,"Discussion,"
59,https://www.reddit.com/r/MachineLearning/comments/e09kjp/discussion_understanding_subscale_wavernn_usage/,e09kjp,discussion_understanding_subscale_wavernn_usage,bigbawsboy,1.0,6,2019-11-23 00:45:06,0,0,"Tech,","Related Paper: Efficient Neural Audio SynthesisI have been reading the sections relating to Subscale WaveRNN in where the DeepMind team was able to generate B samples in a single step. They have discussed about conditioning a particular sample using past samples and up to F samples from the previous sub-tensors future context. In their case, they used a masked dilated CNN (this can be found on the last paragraph of 4.1 Subscale Dependency Scheme). Here's the excerpt specifically to this:The Subscale WaveRNN that generates a given sub-tensor is conditioned on the future context of previous sub-tensors using a masked dilated CNN with relus and the mask applied over past connections instead of future ones.My first question is: how could a masked dilated CNN help with this?Next, Nal Kalchbrenner has tweeted this quick demo of the Subscale WaveRNN. This one confuses me a lot when I'm referring back to the original paper.My final question is: does anyone have taken a look at subscaling more closely?Any insights would be appreciated.(Note: This is my first post and I am hoping that I followed the format correctly.)",,"Discussion,"
60,https://www.reddit.com/r/MachineLearning/comments/e04xzg/r_hybrid_composition_with_idleblock_more/,e04xzg,r_hybrid_composition_with_idleblock_more,ZihengJiang,0.92,11,2019-11-22 19:21:17,0,1,"Tech,",,https://arxiv.org/abs/1911.08609,"Research,"
61,https://www.reddit.com/r/MachineLearning/comments/e02grw/r_deep_neuroethology_of_a_virtual_rodent/,e02grw,r_deep_neuroethology_of_a_virtual_rodent,hardmaru,0.91,8,2019-11-22 16:27:29,0,5,"Tech,",,https://arxiv.org/abs/1911.09451,"Research,"
62,https://www.reddit.com/r/MachineLearning/comments/dzwm9r/r_efficientdet_scalable_and_efficient_object/,dzwm9r,r_efficientdet_scalable_and_efficient_object,hardmaru,0.92,46,2019-11-22 06:37:28,0,10,"Tech,",,https://arxiv.org/abs/1911.09070,"Research,"
63,https://www.reddit.com/r/MachineLearning/comments/dzzjzb/d_uncertainty_estimation_in_dl/,dzzjzb,d_uncertainty_estimation_in_dl,Maplernothaxor,0.78,7,2019-11-22 12:09:03,0,5,"Tech,",Any interesting papers pushing the boundaries of creating well calibrated uncertainties with neural networks (with minimal computational expense ideally)?,,"Discussion,"
64,https://www.reddit.com/r/MachineLearning/comments/dzzw5c/d_an_open_source_stack_for_managing_and_deploying/,dzzw5c,d_an_open_source_stack_for_managing_and_deploying,thumbsdrivesmecrazy,0.77,7,2019-11-22 12:46:04,0,1,"Tech,","https://towardsdatascience.com/an-open-source-stack-for-managing-and-deploying-models-c5d3b98160bcIn this tutorial, we’re going to use DVC to create a model capable of analyzing StackOverflow posts, and recognizing which ones are about Python. We are then going to deploy our model as a web API, ready to form the backend of a piece of production software.DVC stores your model weights and training data in a centralized location, allowing collaborators to get started easily, while also tracking changes and ensuring an accurate version history.As a final step in this tutorial, we’re going to integrate DVC with another open source tool—Cortex—that allows us to deploy DVC-generated models as web APIs, ready for production.",,"Discussion,"
65,https://www.reddit.com/r/MachineLearning/comments/dzmssp/d_why_does_hierarchical_bayesian_regression_work/,dzmssp,d_why_does_hierarchical_bayesian_regression_work,paulie007,0.97,172,2019-11-21 18:38:42,0,34,"Tech,","I have a dataset of electrical outages and it is extremely imbalanced, <2% of all of the data are positive classes. I am using weather station data to try to predict the probability of an outage occurring near the weather stations.When I try any other model I have to rebalance the data to get any good results. However I have recently tried hierarchical Bayesian logistic regression and it performs just fine without resampling. In my methodology every individual weather station has a unique intercept and coefficients, but they are each drawn from a parent distribution.What I would like to discuss is why does the hierarchical approach perform so much better on the imbalanced dataset?",,"Discussion,"
66,https://www.reddit.com/r/MachineLearning/comments/e020vo/n_how_to_convert_a_nn_model_from_tensorflow_lite/,e020vo,n_how_to_convert_a_nn_model_from_tensorflow_lite,xmartlabs,0.6,1,2019-11-22 15:54:13,0,1,"Tech,","Hey everyone,We recently put together this blogpost, to show you how to convert a NN model from TensorFlow Lite to CoreMLHope folks find something helpful here!https://blog.xmartlabs.com/2019/11/22/TFlite-to-CoreML/",,"News,"
67,https://www.reddit.com/r/MachineLearning/comments/dzs00o/p_openai_safety_gym/,dzs00o,p_openai_safety_gym,hardmaru,0.86,14,2019-11-22 00:23:31,0,12,"Tech,","From the project page:Safety GymWe’re releasing Safety Gym, a suite of environments and tools for measuring progress towards reinforcement learning agents that respect safety constraints while training. We also provide a standardized method of comparing algorithms and how well they avoid costly mistakes while learning. If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learning—like a self-driving car that can learn to avoid accidents without actually having to experience them.https://openai.com/blog/safety-gym/",,"Project,"
68,https://www.reddit.com/r/MachineLearning/comments/dzsssi/d_voice_assistant_better_to_use_a_model_trained/,dzsssi,d_voice_assistant_better_to_use_a_model_trained,elmosworld37,0.82,10,2019-11-22 01:22:58,0,20,"Tech,","I would like to make a deep-learning based voice assistant for an application I have that controls a digital camera. Some example commands are ""auto focus"", ""set zoom to 2"", ""turn off flash"", etc.I see two ways of going about this:Train a model that classifies an audio snippet as containing one of the commands or background noise. This seems easier than option 2 but also less robust, as I would have to retrain the model every time I add a new command. Also not sure how numbers would work (record myself saying every number up to like 100?).Use STT to convert audio to text and do some fuzzy string matching to see if it matches a command. I've downloaded Mozilla's DeepSpeech and it did not seem to work very well, so I'm guessing that creating a good STT model is very difficult.Which of these is a better approach? Or is there some in-between approach that's even better?Edit: no cloud solutions please, I want to keep everything offline.",,"Discussion,"
69,https://www.reddit.com/r/MachineLearning/comments/dzimhf/r_video_analysis_muzero_mastering_atari_go_chess/,dzimhf,r_video_analysis_muzero_mastering_atari_go_chess,ykilcher,0.89,55,2019-11-21 13:29:49,0,11,"Tech,","https://youtu.be/We20YSAJZSEMuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren't available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations' latent representations, and thus learns to only represent things that matter to the task!Abstract:Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.Authors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silverhttps://arxiv.org/abs/1911.08265",,"Research,"
70,https://www.reddit.com/r/MachineLearning/comments/dzwrk1/d_ai_to_monitor_network/,dzwrk1,d_ai_to_monitor_network,nvitaly,0.5,0,2019-11-22 06:52:12,0,14,"Tech,","Hello,I have monitoring system watching for bandwidth, connections and connections rates from multiple firewalls, which is stream of counters with interval 5 min.My current system create baseline from data for last 4 weeks and compare current value with baseline. It is ok but it either give me lots of false alerts or too slow to react without additional triggers. Is there anything better available today ? Some system I can feed data in that will learn patterns and identify outages in real time.Open source, but that I can use without getting into machine learning theory too deep just to start using it :)Thank you",,"Discussion,"
71,https://www.reddit.com/r/MachineLearning/comments/dzovrl/d_is_the_python_statsmodels_library_production/,dzovrl,d_is_the_python_statsmodels_library_production,AlexSnakeKing,0.79,5,2019-11-21 20:57:11,0,8,"Tech,","I've had success with sklearn in production, as well as with TF.Is statsmodels something that can run in production? Its documentation doesn't seem up to par but its code base doesn't look bad.",,"Discussion,"
72,https://www.reddit.com/r/MachineLearning/comments/dzakrs/r_191108265_mastering_atari_go_chess_and_shogi_by/,dzakrs,r_191108265_mastering_atari_go_chess_and_shogi_by,ankeshanand,0.98,213,2019-11-21 00:54:00,0,89,"Tech,",,https://arxiv.org/abs/1911.08265,"Research,"
73,https://www.reddit.com/r/MachineLearning/comments/e02lq4/d_what_are_some_interesting_questions_we_can/,e02lq4,d_what_are_some_interesting_questions_we_can,mrsailor23,0.24,0,2019-11-22 16:37:22,0,1,"Tech,","Hi everyone,I have created a simple concept: “Train an AI to answer everyday questions and upload the simulations online”I already created the first project/simulation a couple of months ago, but I’m now looking for the next questions we could answer using AI. I have a few ideas like:“A.I. Learns: Is it better to walk or run in the rain?”“A.I. Learns: How to make a profit from cryptocurrency”etc…..Topics could be serious or lighter and funny sometimes :)So, what are some interesting questions we can answer using AI?I’ll pick up interesting answers for my next project.Thank you.",,"Discussion,"
74,https://www.reddit.com/r/MachineLearning/comments/dzmml5/d_combining_nontext_features_with_text_classifier/,dzmml5,d_combining_nontext_features_with_text_classifier,saint----,0.81,9,2019-11-21 18:26:57,0,5,"Tech,","Hi! So I'm building a classifier which primarily looks at text, but I also want to include other features, which are non-text, and I was wondering what is the best way to do it? I feel like just adding another dimension in the vector which represents the text might cause these features to get 'lost', but maybe that's not true. Is ther there some sort of agreed upon way of including these additional non-text features in? By non-text I mean just information which is not part of the body of the text, like some other meta data.Thanks!",,"Discussion,"
75,https://www.reddit.com/r/MachineLearning/comments/dzhgyg/d_does_efficientnet_really_help_in_real_projects/,dzhgyg,d_does_efficientnet_really_help_in_real_projects,___mlm___,0.91,26,2019-11-21 11:37:20,0,14,"Tech,","There are large amount of papers which show that EfficientNet improves some CV  tasks e.g. EfficientDet: Scalable and Efficient Object Detection.But does it help much in real projects ? Do you guys have any experience with that ?One more thing - ImageNet or COCO datasets are far away from what we have to deal with in real projects. Usually we have only small amount of images/classes, so improvements for COCO/ImageNet != improvements for real projects. What do you think ?",,"Discussion,"
